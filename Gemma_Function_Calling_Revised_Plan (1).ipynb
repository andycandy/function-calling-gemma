{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "yWjyCOtQhGfC",
        "t3_9x4xnhLu5"
      ]
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a78a04397434b2ebc3a7dc1fe837f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6c3ffda13d6418aa15d7b06dcfa41e6",
              "IPY_MODEL_c47c9dd66e654ae4b4d322bbf18b0696",
              "IPY_MODEL_60feeb95c5cf42e488621892f5af8909"
            ],
            "layout": "IPY_MODEL_fabd4345e1cf405d9f3b1aa1f762d320"
          }
        },
        "d6c3ffda13d6418aa15d7b06dcfa41e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10dc2c2c90f94557a18ae5b28e48638b",
            "placeholder": "​",
            "style": "IPY_MODEL_f5a82f7442e543ae98e08b12fbeda5c7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c47c9dd66e654ae4b4d322bbf18b0696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2647cf9095ac4444ae111655552b993c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ef1e2817e2949d3a60ea7ee474ebd2c",
            "value": 2
          }
        },
        "60feeb95c5cf42e488621892f5af8909": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f48aec386c9345a08f19ad9045616751",
            "placeholder": "​",
            "style": "IPY_MODEL_c9190ff759d24adcb795160a1adc5884",
            "value": " 2/2 [00:43&lt;00:00, 22.28s/it]"
          }
        },
        "fabd4345e1cf405d9f3b1aa1f762d320": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10dc2c2c90f94557a18ae5b28e48638b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5a82f7442e543ae98e08b12fbeda5c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2647cf9095ac4444ae111655552b993c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ef1e2817e2949d3a60ea7ee474ebd2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f48aec386c9345a08f19ad9045616751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9190ff759d24adcb795160a1adc5884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4eAfWiaVqJg"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Please ensure you have imported a Gemini API key from AI Studio.\n",
        "You can do this directly in the Secrets tab on the left.\n",
        "\n",
        "After doing so, please run the setup cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yh0uz8f4VqJi",
        "outputId": "64d23eef-2036-4744-ee7e-a74880dafa46"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-mcfvshy9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-mcfvshy9\n",
            "  Resolved https://github.com/huggingface/transformers to commit b7fc2daf8b3fe783173c270d592073aabfb426cb\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.51.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.51.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.0.dev0) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj1p_xSlVqJj"
      },
      "source": [
        "# Generated Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHnctrCcVqJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7564a50b-06a0-4fdb-b773-f2a6b5e967d9"
      },
      "source": [
        "!pip install -U \"bitsandbytes\"\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.4)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login -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"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuKFf_fNfq0G",
        "outputId": "e35c0e0e-f6dc-43fe-b5d1-03b1a49b917c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `tempo` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `tempo`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, Gemma3ForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "ckpt = \"google/gemma-3-4b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 # Or torch.float16\n",
        ")\n",
        "\n",
        "model = Gemma3ForCausalLM.from_pretrained(\n",
        "    ckpt,\n",
        "    torch_dtype = torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3a78a04397434b2ebc3a7dc1fe837f8e",
            "d6c3ffda13d6418aa15d7b06dcfa41e6",
            "c47c9dd66e654ae4b4d322bbf18b0696",
            "60feeb95c5cf42e488621892f5af8909",
            "fabd4345e1cf405d9f3b1aa1f762d320",
            "10dc2c2c90f94557a18ae5b28e48638b",
            "f5a82f7442e543ae98e08b12fbeda5c7",
            "2647cf9095ac4444ae111655552b993c",
            "9ef1e2817e2949d3a60ea7ee474ebd2c",
            "f48aec386c9345a08f19ad9045616751",
            "c9190ff759d24adcb795160a1adc5884"
          ]
        },
        "id": "rCKTJkfDZJfE",
        "outputId": "69756c54-35d8-4c84-8c77-e2d1343add98"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a78a04397434b2ebc3a7dc1fe837f8e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "kO5tyjXlsy8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "setup_instructions = \"\"\"\n",
        "You are a highly competent and professional assistant with access to a set of specialized tools. Your task is to respond to user requests.\n",
        "If a user request requires invoking one or more tools, you MUST respond exclusively by putting function calls in the following JSON format:\n",
        "\n",
        "{ \"function_calls\": [ {\"name\": \"function_name\", \"parameters\": { ... }}, {\"name\": \"function_name\", \"parameters\": { ... }} ] }\n",
        "\n",
        "WHILE ANSWERING TO FUNCTION CALLS DO NOT REPLY WITH ANY OTHER TEXT.\n",
        "Only include valid JSON function calls as defined in the available tools.\n",
        "\n",
        "If no tools are required simply answer the query in a normal way.\n",
        "\"\"\"\n",
        "function_definitions_list = [\n",
        "    {\n",
        "        \"name\": \"get_current_weather\",\n",
        "        \"description\": \"Get the current weather conditions for a specific location.\",\n",
        "        \"parameters\": { # Add parameters schema - crucial for the model!\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The city and state, e.g., San Francisco, CA\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"location\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"send_message\",\n",
        "        \"description\": \"Sends a message to a recipient.\",\n",
        "        \"parameters\": { # Add parameters schema\n",
        "             \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"recipient\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The email address or identifier of the recipient.\"\n",
        "                },\n",
        "                \"body\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The content of the message.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"recipient\", \"body\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "function_definitions_json_string = json.dumps(function_definitions_list, indent=2)\n",
        "\n",
        "user_query = \"Can you check the current weather in odisha\"\n",
        "\n",
        "final_user_content = f\"\"\"{setup_instructions}\n",
        "\n",
        "Available Tools:\n",
        "{function_definitions_json_string}\n",
        "\n",
        "User Query: {user_query}\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "fRPdrY20nWMo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parser"
      ],
      "metadata": {
        "id": "yWjyCOtQhGfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def parse_tool_calls(decoded_output: str) -> list | None:\n",
        "    \"\"\"\n",
        "    Parses the specific '{\"function_calls\": [...] }' JSON format,\n",
        "    handling potential markdown fences. Returns a list of tool call dicts\n",
        "    or None if no valid structure is found.\n",
        "    \"\"\"\n",
        "    cleaned_output = decoded_output.strip()\n",
        "    match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", cleaned_output, re.DOTALL | re.IGNORECASE)\n",
        "    if match:\n",
        "        json_string = match.group(1)\n",
        "    else:\n",
        "        json_string = cleaned_output.replace(\"<end_of_turn>\", \"\").strip()\n",
        "\n",
        "    if not json_string.startswith('{') or not json_string.endswith('}'):\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        parsed_json = json.loads(json_string)\n",
        "    except json.JSONDecodeError as e:\n",
        "        # Keep error logging during development, can be replaced by proper logging later\n",
        "        print(f\"Parser Error: Failed to decode JSON: {e}\")\n",
        "        return None\n",
        "\n",
        "    if isinstance(parsed_json, dict) and \"function_calls\" in parsed_json:\n",
        "        function_calls_list = parsed_json[\"function_calls\"]\n",
        "        if isinstance(function_calls_list, list):\n",
        "            validated_calls = []\n",
        "            for call in function_calls_list:\n",
        "                if isinstance(call, dict) and \"name\" in call and \"parameters\" in call:\n",
        "                    validated_calls.append(call)\n",
        "                else:\n",
        "                    # Invalid item found in the list, treat as failure for strictness\n",
        "                    print(f\"Parser Error: Invalid item structure in function_calls list: {call}\")\n",
        "                    return None # Or decide to skip invalid items and return partial list\n",
        "            return validated_calls # Return the list of validated calls\n",
        "        else:\n",
        "            print(\"Parser Error: 'function_calls' key did not contain a list.\")\n",
        "            return None\n",
        "    else:\n",
        "        # JSON was valid but didn't match the expected top-level structure\n",
        "        return None"
      ],
      "metadata": {
        "id": "CI7KuSk89BFO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing function calls"
      ],
      "metadata": {
        "id": "t3_9x4xnhLu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "parsed_calls = parse_tool_calls(decoded)\n",
        "\n",
        "if parsed_calls is not None:\n",
        "  print(\"Successfully parsed function calls:\")\n",
        "  for call in parsed_calls:\n",
        "    print(f\" Name: {call['name']}, Parameters: {call['parameters']}\")\n",
        "else:\n",
        "  print(\"Failed to parse function calls.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_0mX8PUP2Dq",
        "outputId": "a217b1d1-6d85-466a-c58a-5913f04d2dc6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully parsed function calls:\n",
            " Name: send_message, Parameters: {'recipient': 'mom', 'body': 'Anand will be home tonight'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining functions"
      ],
      "metadata": {
        "id": "fPOIs6fuhS1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Gets the current weather for a location.\"\"\"\n",
        "    print(f\"--- TOOL EXECUTING: get_current_weather(location='{location}') ---\")\n",
        "    if \"new york\" in location.lower():\n",
        "        return json.dumps({\"location\": location, \"temperature\": \"95F\", \"condition\": \"Sunny\"})\n",
        "    elif \"san francisco\" in location.lower():\n",
        "         return json.dumps({\"location\": location, \"temperature\": \"60F\", \"condition\": \"Foggy\"})\n",
        "    else:\n",
        "        return json.dumps({\"location\": location, \"temperature\": \"80F\", \"condition\": \"Rainy\"})\n",
        "\n",
        "def send_message(recipient: str, body: str) -> str:\n",
        "    \"\"\"Sends a message to a recipient.\"\"\"\n",
        "    print(f\"--- TOOL EXECUTING: send_message(recipient='{recipient}', body='{body}') ---\")\n",
        "    # Simulate success\n",
        "    return json.dumps({\"status\": \"Message sent successfully\", \"to\": recipient})"
      ],
      "metadata": {
        "id": "iH2KRUUXgyNX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "available_tools = {\n",
        "    \"get_current_weather\": get_current_weather,\n",
        "    \"send_message\": send_message,\n",
        "    # Add other function names and their corresponding Python function objects here\n",
        "}"
      ],
      "metadata": {
        "id": "F-DgH1s5h0qA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executing"
      ],
      "metadata": {
        "id": "26wf9gnpomH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Executor Logic ---\n",
        "\n",
        "def execute_tool_calls(parsed_tool_calls: list, tool_registry: dict) -> list:\n",
        "    \"\"\"\n",
        "    Executes a list of tool calls based on the parsed output and a registry.\n",
        "\n",
        "    Args:\n",
        "        parsed_tool_calls: The list of dicts from the parser, e.g.,\n",
        "                           [{\"name\": \"...\", \"parameters\": {...}}, ...].\n",
        "                           Assumes this is not None (checked before calling).\n",
        "        tool_registry: A dictionary mapping tool names (str) to callable functions.\n",
        "\n",
        "    Returns:\n",
        "        A list of results from executing each tool call. Each result is often\n",
        "        stored as a dict containing the original call info and the output.\n",
        "        Returns an empty list if the input list was empty.\n",
        "    \"\"\"\n",
        "    execution_results = []\n",
        "\n",
        "    if not parsed_tool_calls:\n",
        "        return execution_results\n",
        "\n",
        "    for tool_call in parsed_tool_calls:\n",
        "        function_name = tool_call.get(\"name\")\n",
        "        parameters = tool_call.get(\"parameters\", {})\n",
        "\n",
        "        if not function_name:\n",
        "            print(\"Executor Error: Tool call missing 'name'. Skipping.\")\n",
        "            execution_results.append({\n",
        "                \"call\": tool_call,\n",
        "                \"error\": \"Missing function name\"\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        if function_name not in tool_registry:\n",
        "            print(f\"Executor Error: Tool '{function_name}' not found in registry. Skipping.\")\n",
        "            execution_results.append({\n",
        "                \"call\": tool_call,\n",
        "                \"error\": f\"Function '{function_name}' not registered.\"\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        function_to_call = tool_registry[function_name]\n",
        "\n",
        "        try:\n",
        "            result = function_to_call(**parameters)\n",
        "            execution_results.append({\n",
        "                \"call\": tool_call, # Store the original call for context\n",
        "                \"output\": result   # Store the function's return value\n",
        "            })\n",
        "            print(f\"Executor: Call to {function_name} succeeded. Result: {result}\")\n",
        "\n",
        "        except TypeError as e:\n",
        "            print(f\"Executor Error: TypeError calling {function_name}: {e}. Check parameters.\")\n",
        "            execution_results.append({\n",
        "                \"call\": tool_call,\n",
        "                \"error\": f\"Parameter mismatch for '{function_name}': {e}\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Executor Error: Exception during execution of {function_name}: {e}\")\n",
        "            execution_results.append({\n",
        "                \"call\": tool_call,\n",
        "                \"error\": f\"Execution error in '{function_name}': {e}\"\n",
        "            })\n",
        "\n",
        "    return execution_results"
      ],
      "metadata": {
        "id": "-46JCJywjhW1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "Wy2YzK5JsiR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TURNS = 5 # Prevent infinite loops\n",
        "turn_count = 0\n",
        "\n",
        "\n",
        "chat_history = [\n",
        "    {\"role\": \"user\", \"content\": final_user_content}\n",
        "]\n",
        "model_inputs = tokenizer.apply_chat_template(\n",
        "      chat_history,\n",
        "      add_generation_prompt=True, # Adds the prompt for the model turn, e.g., <start_of_turn>model\\n\n",
        "      tokenize=True,\n",
        "      return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "\n",
        "input_len = model_inputs.shape[1]\n",
        "with torch.inference_mode():\n",
        "  generation = model.generate(input_ids = model_inputs, max_new_tokens=150, do_sample=False)\n",
        "  generation = generation[0][input_len:]\n",
        "decoded = tokenizer.decode(generation, skip_special_tokens = True)\n",
        "chat_history.append({\"role\": \"assistant\", \"content\": decoded})\n",
        "\n",
        "\n",
        "while turn_count < MAX_TURNS:\n",
        "    turn_count += 1\n",
        "    print(f\"\\n--- Agent Turn {turn_count} ---\")\n",
        "\n",
        "    model_inputs = tokenizer.apply_chat_template(\n",
        "        chat_history,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Clear input length calculation (needed inside loop if history changes)\n",
        "    # Note: This might need adjustment depending on how padding/truncation is handled\n",
        "    input_len = model_inputs.shape[1]\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        generation = model.generate(input_ids=model_inputs, max_new_tokens=200, do_sample=False)\n",
        "        # Ensure we decode only the newly generated tokens\n",
        "        # Be careful with slicing if using padding\n",
        "        new_generation_ids = generation[0][input_len:]\n",
        "        decoded = tokenizer.decode(new_generation_ids, skip_special_tokens=True) # Skip special tokens for cleaner parsing/output\n",
        "\n",
        "    print(f\"Model Output (Turn {turn_count}):\\n{decoded}\")\n",
        "\n",
        "    # 3. Append model's response to history IMMEDIATELY\n",
        "    # We store the raw response that *might* contain function calls\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": decoded})\n",
        "\n",
        "    # 4. Parse for tool calls in the *latest* model response\n",
        "    parsed_calls = parse_tool_calls(decoded)\n",
        "\n",
        "    if parsed_calls:\n",
        "        print(\"\\n--- Function Calls Identified ---\")\n",
        "        print(json.dumps(parsed_calls, indent=2))\n",
        "        print(\"\\n--- Executing Tools ---\")\n",
        "        tool_results = execute_tool_calls(parsed_calls, available_tools)\n",
        "        print(\"\\n--- Tool Execution Results ---\")\n",
        "        print(json.dumps(tool_results, indent=2))\n",
        "\n",
        "        # --- CRUCIAL: Feed results back correctly ---\n",
        "        # This is the part that needs verification/experimentation based\n",
        "        # on Gemma 3's specific chat template capabilities in transformers.\n",
        "        # Option A (Conceptual - requires checking actual template support):\n",
        "        # Try to use role: \"tool\" if supported. Might need tool_call_id.\n",
        "        # for i, result in enumerate(tool_results):\n",
        "        #     # Assuming parser gives unique IDs or we can generate them\n",
        "        #     tool_call_id = parsed_calls[i].get(\"id\", f\"call_{turn_count}_{i}\")\n",
        "        #     content = result.get(\"output\", json.dumps({\"error\": result.get(\"error\")}))\n",
        "        #     chat_history.append({\n",
        "        #         \"role\": \"tool\",\n",
        "        #         \"tool_call_id\": tool_call_id, # Might be needed\n",
        "        #         \"content\": content\n",
        "        #     })\n",
        "\n",
        "\n",
        "        # Option C (Simpler Workaround - closer to your code):\n",
        "        # Directly append the results JSON, maybe with a preamble.\n",
        "        # Still using 'user' role here as in your example, BUT BE AWARE this might confuse the model.\n",
        "        # Testing is needed!\n",
        "        results_content = json.dumps(tool_results, indent=2)\n",
        "        chat_history.append({\"role\": \"user\", \"content\": f\"Tool results:\\n{results_content}\"})\n",
        "        # --- End Crucial Section ---\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"\\n--- No Function Calls Identified: Assuming Final Answer ---\")\n",
        "        final_response_to_user = decoded\n",
        "        break\n",
        "\n",
        "if turn_count >= MAX_TURNS:\n",
        "    print(\"\\n--- Reached Max Turns ---\")\n",
        "    final_response_to_user = chat_history[-1]['content'] if chat_history[-1]['role'] == 'assistant' else \"Reached max turns without final answer.\"\n",
        "\n",
        "# --- Output the final result ---\n",
        "print(\"\\n--- Final Response to User ---\")\n",
        "print(final_response_to_user)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNHU3TL1qJze",
        "outputId": "cb461240-4f69-4778-cd8a-93d2fdef9059"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Agent Turn 1 ---\n",
            "Model Output (Turn 1):\n",
            "\n",
            "\n",
            "--- No Function Calls Identified: Assuming Final Answer ---\n",
            "\n",
            "--- Final Response to User ---\n",
            "\n"
          ]
        }
      ]
    }
  ]
}